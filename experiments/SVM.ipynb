{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78049da6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78049da6",
        "outputId": "a62c9334-4c25-451e-db6b-5e7a6908726e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path=\"hamvsspam.csv\"\n",
        "\n",
        "df = pd.read_csv(\"hamvsspam.csv\", encoding='latin1')\n",
        "\n",
        "# Keep only the first two columns (label and message)\n",
        "df = df.iloc[:, :2]\n",
        "\n",
        "# Rename the columns\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc54a0c",
      "metadata": {
        "id": "0bc54a0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "X=df['message']\n",
        "y=df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7965314",
      "metadata": {
        "id": "d7965314"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing"
      ],
      "metadata": {
        "id": "37S2l5FLBDZd"
      },
      "id": "37S2l5FLBDZd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect emoji contractions textacy nltk\n",
        "\n",
        "from autocorrect import Speller\n",
        "from emoji import demojize\n",
        "from contractions import fix\n",
        "from textacy.preprocessing.remove import accents\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize necessary tools\n",
        "speller = Speller()\n",
        "stopword = stopwords.words(\"english\")\n",
        "stem = SnowballStemmer(\"english\")\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def text_pre_processing(text):\n",
        "    # Lower case the text\n",
        "    text = text.lower()\n",
        "    # Auto Correct\n",
        "    text = speller.autocorrect_sentence(text)\n",
        "    # Emoji to text\n",
        "    text = demojize(text)\n",
        "    # Fix contractions\n",
        "    text = fix(text)\n",
        "    # Remove accents\n",
        "    text = accents(text)\n",
        "    # Remove non-alphanumeric characters (punctuation)\n",
        "    text = re.sub(r\"[^a-z0-9]\", \" \", text)\n",
        "\n",
        "    # Tokenizing and applying stop words, stemming, and lemmatizing\n",
        "    words = word_tokenize(text)\n",
        "    new_text = []\n",
        "    for word in words:\n",
        "        if word not in stopword:\n",
        "            word = stem.stem(word)\n",
        "            word = lem.lemmatize(word)\n",
        "            new_text.append(word)\n",
        "\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "data = pd.read_csv(\"hamvsspam.csv\", encoding='latin1')\n",
        "# Keep only the first two columns (label and message)\n",
        "data = data.iloc[:, :2]\n",
        "\n",
        "# Rename the columns\n",
        "data.columns = ['label', 'message']\n",
        "\n",
        "# Apply the text preprocessing to the 'Message' column\n",
        "data[\"message\"] = data[\"message\"].apply(text_pre_processing)\n",
        "\n",
        "# Save the processed dataset to a new CSV file\n",
        "data.to_csv(\"processed_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "EpQ6crgK_e-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea70c16-6991-41fe-cd24-31aa37d08f9f"
      },
      "id": "EpQ6crgK_e-j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/622.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (5.5.2)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (1.4.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from textacy) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (2.0.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (1.6.1)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.11/dist-packages (from textacy) (3.8.5)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.11/dist-packages (from textacy) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->textacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->textacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->textacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->textacy) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->textacy) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (2.5.1)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (0.15.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy~=3.0->textacy) (3.5.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy~=3.0->textacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.1.2)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading floret-0.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (356 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.9/356.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=f99741163999376338b73c8c1b2110b47a3fde0d351b81c055741fff0e5c80f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/90/99/807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: pyphen, pyahocorasick, jellyfish, floret, emoji, cytoolz, autocorrect, anyascii, textsearch, contractions, textacy\n",
            "Successfully installed anyascii-0.3.2 autocorrect-2.6.1 contractions-0.1.73 cytoolz-1.0.1 emoji-2.14.1 floret-0.10.5 jellyfish-1.2.0 pyahocorasick-2.1.0 pyphen-0.17.2 textacy-0.13.0 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Vectorization"
      ],
      "metadata": {
        "id": "aBqWFHTdBHpm"
      },
      "id": "aBqWFHTdBHpm"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "label = data['label']\n",
        "features = data['message']\n",
        "\n",
        "# Split the data into training and testing sets (optional but common)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize CountVectorizer (Bag-of-Words)\n",
        "bow = CountVectorizer(stop_words=\"english\", strip_accents=\"unicode\")\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "\n",
        "# Transform test data (if needed later)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "# Convert to DataFrame for inspection or further processing\n",
        "X_train_vec = pd.DataFrame(X_train_bow.toarray(), columns=bow.get_feature_names_out())\n",
        "y_train_vec = pd.DataFrame(y_train.reset_index(drop=True)).rename(columns={0: \"label\"})\n"
      ],
      "metadata": {
        "id": "b6JHclNvBUn8"
      },
      "id": "b6JHclNvBUn8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear')  # You can try other kernels like 'rbf' or 'poly'\n",
        "svm_clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict on train set\n",
        "y_pred_train = svm_clf.predict(X_train_bow)\n",
        "\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred_train))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_test = svm_clf.predict(X_test_bow)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "rJmMoCO1DH7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1efcc045-2871-4ba1-ec9a-ab2ca9d275d4"
      },
      "id": "rJmMoCO1DH7w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9998370804822417\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      1.00      1.00      5321\n",
            "        spam       1.00      1.00      1.00       817\n",
            "\n",
            "    accuracy                           1.00      6138\n",
            "   macro avg       1.00      1.00      1.00      6138\n",
            "weighted avg       1.00      1.00      1.00      6138\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5321    0]\n",
            " [   1  816]]\n",
            "Test Accuracy: 0.9895765472312703\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      1.00      0.99      1311\n",
            "        spam       0.99      0.94      0.96       224\n",
            "\n",
            "    accuracy                           0.99      1535\n",
            "   macro avg       0.99      0.97      0.98      1535\n",
            "weighted avg       0.99      0.99      0.99      1535\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1309    2]\n",
            " [  14  210]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}